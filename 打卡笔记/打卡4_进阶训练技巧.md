# PyTorch进阶训练技巧

## 自定义损失函数

### 损失函数常见参数：

- `weight`: 每个类别的loss设置权重
- `reduction`: `none`逐个元素计算| `mean'`加权平均| `sum`(各样本之和)
  - `reduce`和`size_average`都deprecated了
- `ignore_index`: 忽略某个类





### 常见损失函数：

<u>Regression Loss:</u>

- Mean Absolute Error (L1 loss) `torch.nn.L1Loss`

  常用于有异常值的回归问题 **more robust to outliers**

- Mean Squared Error (L2 loss) `torch.nn.MSELoss`

  large mistake produce even larger errors than small errors-->**punishes the model for making big mistakes**

- 平滑L1 (Smooth L1) `torch.nn.SmoothL1Loss` 减少离群点带来的影响

  - combines benefit of MSE loss and MAE loss through a heuristic value $\beta$ 
  - 当$|\text{ground truth}-\text{predicted value}|< \beta$: use sequared difference--会导致gradient explodes for large loss-->switching to MAE

  ![smooth_l1]()

<u>Classification Loss:</u>

- Negative Log-Likelihood Loss `torch.nn.NLLLoss`--最后激活层有softmax 则用NLL; 如果没有 用Cross Entropy
  - model is punished for making the **correct prediction with smaller probabilities** and encouraged for making the prediction with higher probabilities

- softmax--计算normalized exponential function of every unit

​	![softmax]()

- Cross Entropy Loss/Log Loss(SAME when used as loss functs for classification models)
  - difference between 2 prob distributions for a given random var--**softmax prob vs true distribution**
  - penalize especially those predictions that are confident and wrong, and correct but 
  - variant: Binary Cross-Entropy (BCE)--used for binary classification models
    - `nn.BCELoss`(input=**normalized** sigmoid prob) and `nn.BCEWithLogitsLoss`(input=raw unnormalized logits)
  - PyTorch: `nn.CrossEntropyLoss`--inputs must be **UNnormalized** raw value (`logits`) + target must be integer class index from 0 instead of one-hot encoded vectors

​		![cross_entropy1]()

​		![cross_entropy2]()



### 两种定义形式

```python
# 以函数形式定义
def my_loss(output, target):
    loss = torch.mean((output - target)**2)
    return loss

# 以类方式定义--将其看作神经网络的一层来对待
class DiceLoss(nn.Module):
    def __init__(self,weight=None,size_average=True):
        super(DiceLoss,self).__init__()
        
    def forward(self,inputs,targets,smooth=1):
        inputs = F.sigmoid(inputs)       
        inputs = inputs.view(-1) #flatten label and prediction tensors
        targets = targets.view(-1)
        intersection = (inputs * targets).sum()                   
        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  
        return 1 - dice

# 使用方法    
criterion = DiceLoss()
loss = criterion(input,targets)



```

## 动态调整学习率--Learning rate Scheduler

- 经过多轮epoch后，发现准确率震荡或loss不再下降--说明当前学习率已不能满足模型调优的需求-->适当的**学习率衰减**策略来改善这种现象

### 官方scheduler

- Learning rate scheduling should be applied **after** optimizer’s update

```python
model = [Parameter(torch.randn(2, 2, requires_grad=True))]
optimizer = SGD(model, 0.1)
scheduler = ExponentialLR(optimizer, gamma=0.9)

for epoch in range(20):
    for input, target in dataset: #这个整体是train的部分
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    scheduler.step()
```

- 下面每个schedule的写法**只是简写**！

`torch.optim.lr_scheduler.LambdaLR`:initial lr times a given function(a funct that computes a **multiplicative factor** given an integer parameter epoch)

- similar to `lr_scheduler.MultiplicativeLR`

```python
# Assuming optimizer has two groups.
lambda1 = lambda epoch: epoch // 30
lambda2 = lambda epoch: 0.95 ** epoch
lmbda = lambda epoch: 0.95
scheduler = LambdaLR(optimizer, lr_lambda=[lambda1, lambda2]) #list of such functs--one for each group in optimizer.param_groups
# scheduler = MultiplicativeLR(optimizer, lr_lambda=lmbda)
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()
    

```



`lr_scheduler.StepLR`: decay learning rate of each param group by `gamma` every `step_size` epochs

```python
# Assuming optimizer uses lr = 0.05 for all groups
# lr = 0.05     if epoch < 30
# lr = 0.005    if 30 <= epoch < 60
# lr = 0.0005   if 60 <= epoch < 90
# ...
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()
```

`lr_scheduler.MultiStepLR`: decay LR of each param group by `gamma` once number of epoch reaches one of the milestones

- 与上面的`Setp`不同--epoch数不是`step_size`的倍数

```python
# Assuming optimizer uses lr = 0.05 for all groups
# lr = 0.05     if epoch < 30
# lr = 0.005    if 30 <= epoch < 80
# lr = 0.0005   if epoch >= 80
scheduler = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)
for epoch in range(100):
    train(...)
    validate(...)
    scheduler.step()
```

`lr_scheduler.ExponentialLR`: decay learning rate by `gamma` every epoch

`lr_scheduler.ReduceLROnPlateau`: reduce learning rate when a metric stopped improving for `patience` number of epochs(learning stagnates)-->reducing the learning rate by a factor of 2-10

```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
scheduler = ReduceLROnPlateau(optimizer, 'min') #min--quantity monitored should be decreasing #new_lr = lr*factor(默认=0.1)
for epoch in range(10):
    train(...)
    val_loss = validate(...)
    # Note that step should be called after validate()
    scheduler.step(val_loss)
```



`lr_scheduler.CyclicLR`: cyclical learning rate policy (CLR)-->[paper](https://arxiv.org/abs/1506.01186): let learning rate cyclically vary btw reasonable boundary values

- 注：changes LR after **every batch**--所以`step` should be called after a batch has been used for training(与其他schedule不同：每个epoch变化一次)

  ```python
  optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
  scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)
  data_loader = torch.utils.data.DataLoader(...)
  for epoch in range(10):
      for batch in data_loader:
          train_batch(...)
          scheduler.step()
  ```

  ![CyclicLR]()

`lr_scheduler.OneCycleLR`: 1cycle policy [paper](https://arxiv.org/abs/1708.07120)

-->anneals the learning rate from an initial learning rate to some maximum learning rate and then from that maximum learning rate to some minimum learning rate much lower than the initial learning rate

- 注：**not chainable**

- 同样也是change LR after every batch--`step` should be called after a batch而不是epoch



很多learning rate schedule是`chaining schedulers`--can be called back-to-back:

- each scheduler is **applied one after the other** on the learning rate obtained by the one preceding it

```python
model = [Parameter(torch.randn(2, 2, requires_grad=True))]
optimizer = SGD(model, 0.1)
scheduler1 = ExponentialLR(optimizer, gamma=0.9) #scheduler1 
scheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1) #scheduler2

for epoch in range(20):
    for input, target in dataset: #batch
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step() #参数更新 per batch
    scheduler1.step() #LR更新 per epoch(也有per batch的)
    scheduler2.step()
```

### 自定义scheduler

- 自定义函数`adjust_learning_rate`来改变`param_group`中的`lr`

- eg: 学习率每30轮下降为原来的1/10（StepLR与这个需求类似：epochs=1--30 LR；epochs=30--60 LR\*0.1）

```python
def adjust_learning_rate(optimizer, epoch):
    lr = args.lr * (0.1 ** (epoch // 30))
    for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    
# 使用--依然after optimizer update
optimizer = torch.optim.SGD(model.parameters(),lr = args.lr,momentum = 0.9)
for epoch in range(10):
    train(...)
    validate(...)
    adjust_learning_rate(optimizer,epoch)
```



## 模型微调

- 参数越多的大型神经网络对数据量的要求越大--过拟合无法避免
  - Soln: **迁移学习**--将从源数据集上学到的知识迁移到目标数据集上
  - 例如在ImageNet数据集上训练的模型可以抽取**较通用的图像特征**：如边缘、纹理、形状和物体组成

### 模型微调fine tune

找到一个同类的别人训练好的模型，直接将pretrained模型拿过来，换成自己的数据，再通过训练调整参数

- PyTorch提供的pretrained模型(在相应的**大型数据集训练好的**)：VGG，ResNet系列，mobilenet系列......
- `pretrained=True`使用在一些数据集上预训练得到的权重

```python
import torchvision.models as models
resnet18 = models.resnet18()
# resnet18 = models.resnet18(pretrained=False)  等价于与上面的表达式
alexnet = models.alexnet()
vgg16 = models.vgg16()
squeezenet = models.squeezenet1_0()
densenet = models.densenet161()
inception = models.inception_v3()
googlenet = models.googlenet()
shufflenet = models.shufflenet_v2_x1_0()
mobilenet_v2 = models.mobilenet_v2()
mobilenet_v3_large = models.mobilenet_v3_large()
mobilenet_v3_small = models.mobilenet_v3_small()
resnext50_32x4d = models.resnext50_32x4d()
wide_resnet50_2 = models.wide_resnet50_2()
mnasnet = models.mnasnet1_0()
```



### 流程：

- 在源数据集上预训练一个神经网络模型--**源模型**
- 创建目标模型--复制了源模型除输出层外的所有模型设计及参数（假设参数包含了在源数据集上学习到的知识，且这些知识同样适用于目标数据集；还假设输出层只跟源数据集的标签相关，因此不采用）
- 为目标模型添加输出层（#=目标数据集的类别个数），并**随机初始化**该层的模型参数
- 从头训练输出层，其余层基于源模型的参数微调

![finetune]()

#### 注意事项：

- 通常PyTorch模型的扩展为`.pt`或`.pth`

- 预训练模型的下载会比较慢（如果默认路径中已经有下载好的模型权重，下次加载就无须下载了）

  - 可以通过[这里](https://github.com/pytorch/vision/tree/main/torchvision/models)去查看**模型的py文件**里面指明的`model_urls`,然后手动下载，再使用`torch.utils.model_zoo.load_url()`设置权重的下载地址`model_dir`
  - 还可以将权重下载下来放到同文件夹中，再加载网络参数

  ```python
  self.model = models.resnet50(pretrained=False)
  self.model.load_state_dict(torch.load('./model/resnet50-19c8e357.pth'))
  ```

- 如果下载失败或中途停止--一定去对应的路径将全中文件删除干净，要不然可能会报错（Linux和Mac的默认下载路径是用户根目录下的.cache文件夹。在Windows下就是C:\Users\<username>\.cache\torch\hub\checkpoint）

### 训练特定层

- 例如 只想提取特征，或为新初始化的层（输出层）计算梯度，其余参数不进行改变
- 需要设置`requires_grad = False`来冻结部分层

```python
def set_parameter_requires_grad(model, feature_extracting):
    if feature_extracting:
        for param in model.parameters():
            param.requires_grad = False

# 使用预训练模型resnet18
# 将1000类改为4类--仅改变最后一层的模型参数 不改变特征提取的参数
import torchvision.models as models
# 先冻结参数的梯度
feature_extract = True
model = models.resnet18(pretrained=True)
set_parameter_requires_grad(model, feature_extract)
# 再对模型输出部分的全连接层进行修改
num_ftrs = model.fc.in_features
model.fc = nn.Linear(in_features=num_ftrs, out_features=4, bias=True) #只有全连接层的参数可以计算梯度，参数更新
```







