# PyTorch简介

https://www.kdnuggets.com/2022/02/packt-pytorch-tensorflow-comparing-popular-machine-learning-frameworks.html

## Pytorch的优势:

> All in Pytorch

- support dynamic computational graphs(debug-friendly) vs TF: static TensorFlow counterparts

  - TF 2.0 applies eager execution that also evaluates operations immediately, but it comes at a cost of reduced efficiency and speed

- State-of-the-Art Model availablity

  - Most SOTA models are in PyTorch

  - HuggingFace: https://huggingface.co/models

    32,900 are in PyTorch vs 2,748 are in TensorFlow

  - Paper With Code: https://paperswithcode.com/trends

    59% in PyTorch vs 13% in TF

- Easier learning curve: more Pythonic & debug-friendly & more straightforward & more **easily customizable than Keras** vs TF: steeper learning curve--low-level implementations of NN structure

## TensorFlow的优势:

- production deployment--PyTorch dominates in research domain vs TensorFlow more mature in production deployment due to its robust deployment framework

- TF deploy models on **servers** using: **TensorFlow Serving** and **TensorFlow Extended (TFX)**

- TF deploy models to serve on mobile and IoT devices: **TensorFlow Lite (TFLite)**

- PyTorch has closed the gap in recent years: naive deployment tools TorchServe and PyTorch Live 

  - **TorchServe**: https://pytorch.org/serve/ 

    deploying trained PyTorch models without having to write custom code

    - provides an easy-to-use **command-line interface** and supports lightweight serving at scale on many environments(Amazon SageMaker, EKS, and Kubernetes)

  - **PyTorch Live**: https://pytorch.org/live/ 

    targets mobile devices, similar to TFLite



# PyTorch安装

Anaconda/miniconda + PyTorch + (IDE: Pycharm/VScode)

<u>Anaconda安装</u>

<u>创建虚拟环境</u>

**Ref**: https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html

1. 方法1：逐个install package

```python
# 环境创建 这里名字起为myenv
conda create --name myenv
# 这样就自动创建一个myenv环境在/envs/路径下

# create an environment with a specific version of a package
conda create -n myenv python=3.9 scipy=0.17.3 astroid babel

```

2. 方法2：create env from an `environment.yml`文件 内容如下

- 第一行是新环境的名称

```
name: myenv
channels:
  - defaults
  - anaconda
  - conda-forge
dependencies:
  - jupyter
  - matplotlib
  - numpy
  - python=3.7
  - ipykernel
  - scikit-learn
  - nltk
  - gensim
create_default_packages:
  - pip
  - ipython
  - scipy=0.15.0
```

- 如果**每次创建**虚拟环境都默认带某些包：`create_default_packages`
- **override** this option at command prompt add flags:`--no-default-packages`

```
conda create --no-default-packages -n myenv python
```

- 创建虚拟环境 with dependencies specified in `env.yml`文件:

```
conda env create -f environment.yml
```

- 创建之后：

```
# 激活新环境
# 包含adding entried to PATH(conda prepends path name "myenv" to system command)
conda activate myenv

# 检查新环境是否被安装正确--当前环境会有*标识
conda info --envs
conda env list

# Inside新环境--install IPython kernel才能在jupyter notebook中使用这个环境
python -m ipykernel install --user --name myenv

# 启动jupyter notebook
jupyter notebook xxxx.ipynb
# 使用正确的环境
click on Kernel -> Change kernel
 
# deactivate
conda deactivate
# 查看环境中都有什么包
conda list -n myenv
# 查看某个特定的包是否被安装
conda list -n myenv scipy
```

- **更新**已有环境: 情况有三--1. core dependency version number update 2. 需要额外的包 3.找到更好的包不再需要旧包了

```
# update contents of environment.yml文件
# 再运行下面的命令
# prune表示让conda remove any dependencies that are no longer required
conda env update --file environment.yml  --prune 
```

- 现有环境export

```
# 先激活
conda activate myenv
# Export your active environment to a new file
conda env export > environment.yml
```

- 删除环境

```
conda remove --name myenv --all
# verify
conda info --envs
```

<u>换源</u>

**pip换源**

- Linux：

  Linux下的换源，我们首先需要在用户目录下新建文件夹`.pip`，并且在文件夹内新建文件`pip.conf`，具体命令如下

```
cd ~
mkdir .pip/
vi pip.conf
```

随后，我们需要在`pip.conf`添加下方的内容:

在`vim`下 输入`i`进入编辑模式，将下面内容粘贴进去，按`ESC`退出编辑模式，输入`:wq`保存并退出

```
[global]
index-url = http://pypi.douban.com/simple
[install]
use-mirrors =true
mirrors =http://pypi.douban.com/simple/
trusted-host =pypi.douban.com
```

- Windows:

  1、文件管理器文件路径地址栏敲：`%APPDATA%` 回车，快速进入 `C:\Users\电脑用户\AppData\Roaming` 文件夹中 2、新建 pip 文件夹并在文件夹中新建 `pip.ini` 配置文件 3、我们需要在`pip.ini` 配置文件内容，我们可以选择使用记事本打开，输入以下内容，并按下ctrl+s保存，在这里我们使用的是豆瓣源为例子

```
[global]
index-url = http://pypi.douban.com/simple
[install]
use-mirrors =true
mirrors =http://pypi.douban.com/simple/
trusted-host =pypi.douban.com
```

**conda换源（清华源）**

TUNA 提供了 Anaconda 仓库与第三方源的镜像，各系统都可以通过修改用户目录下的 `.condarc` 文件。Windows 用户无法直接创建名为 `.condarc` 的文件，可先执行`conda config --set show_channel_urls yes`生成该文件之后再修改。

完成这一步后，我们需要修改`C:\Users\User_name\.condarc`这个文件，打开后将文件里原始内容删除，将下面的内容复制进去并保存。

```
channels:
  - defaults
show_channel_urls: true
default_channels:
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r
  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2
custom_channels:
  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
  simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud
```

这一步完成后，我们需要打开`Anaconda Prompt` 运行 `conda clean -i` 清除索引缓存，保证用的是镜像站提供的索引。

<u>查看显卡</u>

在`cmd/terminal中`输入`nvidia-smi`或使用**NVIDIA控制面板**和**任务管理器**查看自己是否有NVIDIA的独立显卡及其型号

<u>安装PyTorch</u>

- 找到自己**显卡版本对应的CUDA版本**--PyTorch是向下兼容的--一定保持**PyTorch和cudatoolkit**版本适配
- 例如 我自己：NVDIA-SMI 466.42--CUDA 11.3.1
- **CUDA**: Nvidia并行运算的平台

在线下载：

- 先激活虚拟环境
- 再输入命令(去掉`-c pytorch`保证使用清华源下载)`conda install pytorch torchvision torchaudio cudatoolkit=11.3`

离线下载：

- **下载地址**：https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/
- 通过上面下载地址，我们需要下载好**对应版本**的pytorch和 torchvision 包，然后打开`Anaconda Prompt`/`Terminal`中，进入我们安装的路径下

```
cd package_location
conda activate env_name

conda install --offline pytorch压缩包的全称（后缀都不能忘记）
conda install --offline torchvision压缩包的全称（后缀都不能忘记）
```

检验是否安装成功

```
# 进入虚拟环境
# 输入python
import torch
torch.cuda.is_available() #是否可以调用cuda 如果能调用GPU--返回True
```

# PyTorch基础知识

Tensor张量

- 基于向量和矩阵的推广
- scalar--0维张量；vector--1维张量；matrix--2维张量
- 3维：时间序列；4维：图像；5维：视频
  - 1个图像：(width, height, channel) = 3D
  - 多个图像：(sample_size, width, height, channel) = 4D
- `torch.Tensor` vs `NumPy.ndarray`: tensor提供GPU计算&自动求梯度等功能

- Tensor的创建 
  - 官方文档：https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html

```python
import torch
# simplest way of create
# default:tensors are populated with 32-bit floating point numbers
# random-looking values when printing
# empty() allocates memory for the tensor, but does not initialize it with any values
	# so what you’re seeing is whatever was in memory at the time of allocation
x = torch.empty(3, 4)

# initialize with some value
torch.manual_seed(1729)
x = torch.rand(4, 3) 
x = torch.zeros(4, 3, dtype=torch.long) #注定shape+数据类型

# 基于已经存在的 tensor，创建新的 tensor
# torch.*_like()
# .empty_like(), .zeros_like(), .ones_like(), .rand_like()
x = x.new_ones(4, 3, dtype=torch.double) #filled with 1,same dtype,same device
x = torch.randn_like(x, dtype=torch.float)

# python collections-->PyTorch tensor
# 注意这里create copy of data！
x = torch.tensor([5.5, 3])  #直接使用数据

# 获取它的维度信息
print(x.size())
print(x.shape)
print(type(x))
```

- NumPy的广播性质依然成立：广播的规则如下
- Each tensor must have at least one dimension - no empty tensors.
- Comparing the dimension sizes of the two tensors,*going from last to first*:
  - Each dimension must be equal, *or*
  - One of the dimensions must be of size 1, *or*
  - The dimension does not exist in one of the tensors

- 例如：(4,3,2) & (3,2);& (3,1);& (1,2)
- 使用场景：tensor of learning weights \* **batch** of input tensors--applying the operation to each instance in the batch separately

## Alter Tensors in Place

- 基本所有的binary/arithmetic operation都会返回一个**copy--新的tensor占用新的一块内存**

```python
a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])
print('a:')
print(a)
print(torch.sin(a))   # this operation creates a new tensor in memory
print(a)              # a has not changed

# 如果想要alter tensor in place
# 去掉intermediate values
# math functions + appended underscore_
b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])
print('\nb:')
print(b)
print(torch.sin_(b))  # note the underscore
print(b)              # b has changed

# in-place arithm functs->是torch.Tensor object的methods 而不是torch模块的functs eg:torch.sin()
# a.add_(b) #calling tensor 'a'被change in place
# a.mul_(b)
```



## Copying Tensors

```python
a = torch.ones(2, 2)
b = a #b只是label of the tensor；NOT copy！

a[0][1] = 561  # we change a...
print(b)       # ...and b is also altered

# 索引得到的也是view（与源tensor共享内存 是同一个tensor 仅仅是改变了对这个张量的观察角度）

# separate copy
# clone()
# 如果源tensor有autograd enabled:then so will the clone
a = torch.ones(2, 2)
b = a.clone()

assert b is not a      # different objects in memory...
print(torch.eq(a, b))  # ...but still with the same contents!

a[0][1] = 561          # a changes...
print(b)               # ...but b is still all ones
```

## Manipulating Tensor Shapes

### Change Number of Dimensions

**增加DIM的use case1:** when passing a <u>single input instance</u>  to model--**expect batches of input**

- eg: 假如有一个模型是作用在3\*226\*226的图片上（3 color channels & 226-pixel square) 模型希望的input shape是`(N,3,226,226)` --`N`=# of images in batch
- 此时如何变成**batch of one**:

```python
a = torch.rand(3, 226, 226)
b = a.unsqueeze(0)

print(a.shape) #torch.Size([3, 226, 226])
print(b.shape) #torch.Size([1, 3, 226, 226])
```

- `unsqueeze()`方法添加一个dimension of **extent 1**
- `unsqueeze(0)` adds it as a **new 0th dimension**
  - batch of 1
- taking advantage of the fact that any dimension of `extent 1` *does not* change the number of elements in the tensor.

**增加DIM的use case2:** 便于广播

```python
# 可以广播
a =     torch.ones(4, 3, 2)
c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a

# 不能广播
a = torch.ones(4, 3, 2)
b = torch.rand(   3)     # trying to multiply a * b will give a runtime error
c = b.unsqueeze(1)       # change to a 2-dimensional tensor, adding new dim at the end
# 可以广播
print(a * c)  # broadcasting works again!
```



**减少DIM的use case**: 模型的输出是20-element vector for each input-->output have shape `(N,20)` 如果是**single-input batch**: output shape: `(1,20)`; 如果想要do some **non-batched** computation with this output-->只想要20-elem的vector

```python
a = torch.rand(1, 20)
print(a.shape) #torch.Size([1, 20])
print(a)

b = a.squeeze(0) 
print(b.shape) #torch.Size([20])
print(b)

c = torch.rand(2, 2)
print(c.shape) #torch.Size([2, 2])

d = c.squeeze(0) #can ONLY squeeze dim of extent 1
print(d.shape) #torch.Size([2, 2])
```

==重点== `squeeze()` and `unsqueeze()` can **only act on dimensions of extent 1** because to do otherwise would <u>change the number of elements</u> in the tensor

